{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the necessary Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "# lxml easy-to-use library for processing XML and HTML\n",
    "import lxml\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "import webbrowser\n",
    "#Beautiful Soup is a Python package for parsing HTML and XML documents.\n",
    "#Beautiful Soup is a library that makes it easy to scrape information from web pages.\n",
    "# The requests.get() method sends a GET request to the specified URL\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[]\n",
    "# The Initial URL of the website we intend to Crawl is mentioned below\n",
    "url= \"https://www.imdb.com/list/ls068010962/\"\n",
    "b.append(\"https://www.imdb.com/list/ls068010962/\")\n",
    "#The get() method sends a GET request to the specified URL\n",
    "page = get(url)\n",
    "soup1 = BeautifulSoup(page.content, 'lxml')\n",
    "c = soup1.find(id=\"main\")\n",
    "#list-pagination contains the information regarding the Number of Pages \n",
    "frame=c.find(\"div\", class_=\"list-pagination\")\n",
    "Line=frame.find(\"span\", class_=\"pagination-range\")\n",
    "x=list(Line)\n",
    "y=x[0]\n",
    "p=list(y.split(\" \"))\n",
    "a=[]\n",
    "for i in range(0,len(p)):\n",
    "    if p[i].isdigit():\n",
    "        a.append(p[i])\n",
    "i=len(b)\n",
    "#Here, list 'a' contains the details of the number of Actors Deatils present in the base URL\n",
    "#and the total number of Celebrity details available. Here a=['1','100','200']\n",
    "# With this list, we will be able to find out the total number of pages we need in order to get all the information.\n",
    "if a[1]<a[2]:\n",
    "    i+=1\n",
    "    b.append(str(url)+\"?page=\"+str(i))\n",
    "    a[1]+=a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a class and define functions to get the data from the website\n",
    "class IMDB(object):\n",
    "    def __init__(self, url):\n",
    "        super(IMDB, self).__init__()\n",
    "            \n",
    "    def manip(self,y):\n",
    "        x=\"\\n\\n\"\n",
    "        c=0\n",
    "        p=list(y.split(\" \"))\n",
    "        for i in range(0,len(p)):\n",
    "                if p[i]==x:\n",
    "                    c+=1\n",
    "                    if c==2:\n",
    "                        return \" \".join(str(s) for s in p[i+1:-1])\n",
    "     # This is the function where data such as Actor Name, Most Recognized Movie, Description, \n",
    "    #Their Traits, Their Image,etc... are scraped and appended into their respective lists\n",
    "    def Dataset_Generation(self,Frame,ActorName,ActorTopMovie,ActorDescription,ActorTraits,Sno,imagestr,s,g):\n",
    "        for Actor,i in zip(Frame,range(s,g)):\n",
    "            # Here Actor is the frame content.Actor.find() is used to find a particular class in the Frame\n",
    "            Line1 = Actor.find(\"h3\", class_=\"lister-item-header\")\n",
    "            # Once Class is identified, The Contents can be extrated as shown below.\n",
    "            #the append() method is used to append the extracted contents to their respective lists.\n",
    "            ActorName.append(Line1.find(\"a\").text.replace(\"\\n\",\"\"))\n",
    "            Sno.append(re.sub(r\"[()]\",\"\", Line1.find_all(\"span\")[-1].text))\n",
    "            Line2=Actor.find(\"p\", class_=\"text-muted text-small\")\n",
    "            ActorTopMovie.append(Line2.find(\"a\").text.replace(\"\\n\",\"\"))\n",
    "            #The text provided as <p> in HTML cannot be extracter directly. Hence, We use the get_text() method to acces the entire text.\n",
    "            #the get_text() method returns a list of length 1.\n",
    "            Line3=Actor.find_all(\"div\", class_=\"lister-item-content\")\n",
    "            text = [e.get_text() for e in Line3]\n",
    "            #this list content is saved in the variable y\n",
    "            y=text[0]\n",
    "            #with the text extracted, we find the Trait of the celebrity. \n",
    "            #Whether the person is an Actor, an Actress, Director, Writer or a Producer.\n",
    "            if \"Actor |\" in y:\n",
    "                ActorTraits.append(\"Actor\")\n",
    "            elif \"Actress |\" in y:\n",
    "                ActorTraits.append(\"Actress\")\n",
    "            elif \"Writer |\" in y:\n",
    "                ActorTraits.append(\"Writer\")\n",
    "            elif \"Director |\" in y:\n",
    "                ActorTraits.append(\"Director\")\n",
    "            elif \"Producer |\" in y:\n",
    "                ActorTraits.append(\"Producer\") \n",
    "            else:\n",
    "                ActorTraits.append(\"--NIL--\")\n",
    "            # To get the description from the text we call the already defined function self.manip()     \n",
    "            ActorDescription.append(self.manip(y))\n",
    "            Line4 = Actor.find(\"div\", class_=\"lister-item-image\")\n",
    "            #In order to Scrap image , we need to get the content in the 'src' of the <img> Tag\n",
    "            response = get(Line4.find('img').get(\"src\"))\n",
    "            #The Src file is opened and the image is converted to RGB\n",
    "            bytes = BytesIO(response.content)\n",
    "            image = Image.open(bytes).convert('RGB')\n",
    "            # The converted imaged is saved into a folder \"Images/\"\n",
    "            image.save(\"Images/\" + str(i) +\".jpg\")\n",
    "            # Now the saved image is appended into the imagestr list\n",
    "            imagestr .append('<img src=\"{}\" > '.format(\"Images/\" + str(i) +\".jpg\"))\n",
    "        \n",
    "    # This is the main function where teh DataFrame is generated and .html file is created\n",
    "    def actorData(self,):\n",
    "        # All the required lists are created\n",
    "        ActorName = []\n",
    "        ActorTopMovie = []\n",
    "        ActorDescription=[]\n",
    "        ActorTraits=[]\n",
    "        Sno = []\n",
    "        imagestr=[]\n",
    "        # A folder named 'Images/' is created to store the scraped Images\n",
    "        if not os.path.exists(\"Images/\"):\n",
    "            os.mkdir(\"Images/\")\n",
    "        s=1\n",
    "        g=101\n",
    "        # For each url, i.e each page we need to scrap, the following process is done and the lists are appended\n",
    "        # In this URL, there are a total of 200 Celebrities information and each page consists of 100 Celebrities data\n",
    "        # Hence, we have to scrap two pages i.e len(b)=2 or there are 2 URL's to scrap\n",
    "        for url in b:\n",
    "            page = get(url)\n",
    "            soup = BeautifulSoup(page.content, 'lxml')\n",
    "            content = soup.find(id=\"main\")\n",
    "            Frame=content.find_all(\"div\", class_=\"lister-item mode-detail\")\n",
    "            self.Dataset_Generation(Frame,ActorName,ActorTopMovie,ActorDescription,ActorTraits,Sno,imagestr,s,g)\n",
    "            s+=100\n",
    "            g+=100         \n",
    "        #Creating the DataFrame using Pandas   \n",
    "        ActorData = pd.DataFrame({'Sno': Sno,'Actor Name':ActorName})\n",
    "        ActorData['Actor Image']=imagestr\n",
    "        ActorData['Best Of Their Works']=ActorTopMovie\n",
    "        ActorData['Actor Description']=ActorDescription\n",
    "        ActorData['Trait']=ActorTraits\n",
    "        #In order to include images into the DataFrame, The Path is required.\n",
    "        def path_to_image_html(path):\n",
    "            return '<img src=\"'+ path + '\" width=\"60\" >'\n",
    "        # The pd.set_option() allows us to customize the DataFrame according to our requirements\n",
    "        pd.set_option('display.max_colwidth', -1)\n",
    "        pd.set_option('display.colheader_justify',\"centre\")\n",
    "        ActorData.style.set_properties(subset=['Actor Image'], **{'width': '140px','height':'209'})\n",
    "        #The HTML file is created . This .html is used to display the generated Database\n",
    "        html=ActorData.to_html(escape=False ,formatters=dict(image=path_to_image_html))\n",
    "        text_file = open(soup.find(\"h1\", class_=\"header\").text.replace(\"\\n\",\"\")+\".html\", \"w\")\n",
    "        text_file.write(html)\n",
    "        text_file.close()\n",
    "        #Opening the .html file in Chrome\n",
    "        webbrowser.open('file:///C:/Users/LASYA/Top%20200%20Best%20Indian%20Actors%20and%20Actresses.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LASYA\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:993: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Creating the Class Object\n",
    "    site1 = IMDB(url)\n",
    "    site1.actorData()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
